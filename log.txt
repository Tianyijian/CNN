dict_keys(['train_x', 'train_y', 'dev_x', 'dev_y', 'test_x', 'test_y'])

-------train--------
9989 9989
['Come', 'on,', 'you', 'know,', 'Thanksgiving.', 'Ooh,', 'you', 'got', 'the', 'bigger', 'half.', "What'd", 'you', 'wish', 'for?']   1
['No,', 'Rach,', 'no.', 'I', 'don?t,', 'I', 'don?t,', 'I', 'don?t']   0

-------dev--------
1109 1109
['Eh,', 'just', 'a', 'tad.']   0
['Fine!']   4

-------test--------
2610 2610
['Yeah!', 'Totally!', 'Totally,', 'and', 'you?']   1
['She?s', 'not', 'pregnant.', 'It?s', 'Rachel.', 'Rachel?s', 'the', 'one', 'who?s', 'pregnant.']   0
====================INFORMATION====================
MODEL: non-static
DATASET: MELD
VOCAB_SIZE: 13802
EPOCH: 100
LEARNING_RATE: 1.0
EARLY_STOPPING: True
SAVE_MODEL: True
MAX_SENT_LEN: 69
CLASS_SIZE: 7
====================INFORMATION====================
====================TRAINING STARTED====================
loading word2vec...
load glove.txt
vocab_num:13802, in w2v: 6506, ratio:0.4713809592812636
run.py:85: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  nn.utils.clip_grad_norm(parameters, max_norm=params["NORM_LIMIT"])
epoch: 1 / train_acc: 0.4849334267694464 / dev_acc: 0.43462578899909826 / test_acc: 0.49386973180076627
New best model!
epoch: 2 / train_acc: 0.5285814395835419 / dev_acc: 0.44274120829576197 / test_acc: 0.4992337164750958
New best model!
epoch: 3 / train_acc: 0.5487035739313244 / dev_acc: 0.45626690712353474 / test_acc: 0.5157088122605364
New best model!
epoch: 4 / train_acc: 0.598458304134548 / dev_acc: 0.48872858431018934 / test_acc: 0.5371647509578544
New best model!
epoch: 5 / train_acc: 0.6351987185904495 / dev_acc: 0.4833183047790803 / test_acc: 0.5432950191570881
epoch: 6 / train_acc: 0.7099809790769847 / dev_acc: 0.5040577096483319 / test_acc: 0.5344827586206896
New best model!
epoch: 7 / train_acc: 0.7630393432776054 / dev_acc: 0.4878268710550045 / test_acc: 0.5340996168582376
epoch: 8 / train_acc: 0.7987786565221744 / dev_acc: 0.49413886384129846 / test_acc: 0.5505747126436782
epoch: 9 / train_acc: 0.8139953949344279 / dev_acc: 0.47610459873760147 / test_acc: 0.5126436781609195
early stopping by dev_acc!
max train acc: 0.7099809790769847 max dev acc: 0.5040577096483319 test acc: 0.5344827586206896
A model is saved successfully as saved_models/MELD_non-static_100.pkl!
====================TRAINING FINISHED====================